---
title: "Model definition"
author: "Lennart OelschlÃ¤ger, Timo Adam and Rouven Michels"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model definition}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## The Hidden Markov Model

HMMs constitute a versatile class of statistical models for time series, see e.g. [@zuc16] for a comprehensive introduction. They predicate that the behavior of the nature can be divided into a finite number of states, where the state that is active cannot be directly observed. However, at each point in time, a data point is observed, which depends on the current state of the nature and thus yields information on the latter. More formally, this concept can be formulated by introducing two stochastic processes:

1. At each time point $t$ of the discrete time space $\{1,\dots,T\}$, an underlying process $(S_t)_t$ selects one state from the state space $\{1,\dots,N\}$. We call $(S_t)_t$ the hidden state process.

2. Depending on which state is active at $t$, one of $N$ distributions $f^{(1)},\dots,f^{(N)}$ generates the observation $X_t$. The process $(X_t)_t$ is called the observed state-dependent process. 

We make the following assumptions on these processes:

1. We assume that $(S_t)_t$ is a time-homogeneous Markov process of first order. The process is therefore identified by its initial distribution $\delta$ and its transition probability matrix (t.p.m.) $\Gamma$.

2. The process $(X_t)_t$ is said to satisfy the conditional independence assumption, i.e.\ conditionally on the current state $S_t$, the observation $X_t$ is independent of all other states and observations.

From a practical point of view, it is reasonable to identify the initial distribution of $(S_t)_t$ with its stationary distribution $\pi$ (which we assume to exist): On the one hand, the hidden state process has been evolving for some time before we start to observe it and hence can be assumed to be stationary. On the other hand, $\pi$ is determined by $\Gamma$ through the equation $\pi\Gamma=\pi$, where setting $\delta=\pi$ reduces the number of parameters that need to be estimated, which is convenient from a computational perspective [@zuc16]. 

In case of financial data, the hidden states can be interpreted as different moods of the market. Even though these moods cannot be observed directly, price changes --- which clearly depend on the current mood of the market --- can be observed. Thereby, using an underlying Markov process, we can detect which mood is active at any point in time and how the different moods alternate. Depending on the current mood, a price change is generated by a different distribution. These distributions characterize the moods in terms of expected return and volatility. 

## Adding a Hierarchical Structure

The HMM can be extended by an hierarchical structure, resulting in the HHMM. Assume that we are dealing with two time series observed on two different time scales. For each observation of the time series on the coarser scale, we have several observations of the times series on the finer scale, e.g.\ monthly observations and corresponding daily observations. Following the concept of HMMs, we can model both state-dependent time series jointly. First, we treat the time series on the coarser scale as stemming from an ordinary HMM, which we refer to as the coarse-scale HMM:

1. At each time point $t$ of the coarse-scale time space $\{1,\dots,T\}$, an underlying process $(S_t)_t$ selects one state from the coarse-scale state space $\{1,\dots,N\}$. We call $(S_t)_t$ the hidden coarse-scale state process.

2. Depending on which state is active at $t$, one of $N$ distributions $f^{(1)},\dots,f^{(N)}$ realizes the observation $X_t$. The process $(X_t)_t$ is called the observed coarse-scale state-dependent process. 

The processes $(S_t)_t$ and $(X_t)_t$ have the same properties as before, namely $(S_t)_t$ is a first-order Markov process and $(X_t)_t$ satisfies the conditional independence assumption. 

Subsequently, we segment the observations of the fine-scale time series into $T$ distinct chunks, each of which contains all data points that correspond to the $t$-th coarse-scale time point. Assuming that we have $T^*$ fine-scale observations on every coarse-scale time point, we face $T$ chunks comprising of $T^*$ fine-scale observations each. The hierarchical structure now evinces itself as we model each of the chunks by one of $N$ possible fine-scale HMMs. Each of the fine-scale HMMs has its own t.p.m.\ $\Gamma^{*(i)}$, initial distribution $\delta^{*(i)}$, stationary distribution $\pi^{*(i)}$, and state-dependent distributions $f^{*(i,1)},\dots,f^{*(i,N^*)}$. Which fine-scale HMM is selected to explain the $t$-th chunk of fine-scale observations depends on the hidden coarse-scale state $S_t$. The $i$-th fine-scale HMM explaining the $t$-th chunk of fine-scale observations consists of the following two stochastic processes:

1. At each time point $t^*$ of the fine-scale time space $\{1,\dots,T^*\}$, the process $(S^*_{t,t^*})_{t^*}$ selects one state from the fine-scale state space $\{1,\dots,N^*\}$. We call $(S^*_{t,t^*})_{t^*}$ the hidden fine-scale state process.

2. Depending on which state is active at $t^*$, one of $N^*$ distributions $f^{*(i,1)},\dots,f^{*(i,N^*)}$ realizes the observation $X^*_{t,t^*}$. The process $(X^*_{t,t^*})_{t^*}$ is called the observed fine-scale state-dependent process. 

The fine-scale processes $(S^*_{1,t^*})_{t^*},\dots,(S^*_{T,t^*})_{t^*}$ and $(X^*_{1,t^*})_{t^*},\dots,(X^*_{T,t^*})_{t^*}$ satisfy the Markov property and the conditional independence assumption, respectively, as well. Furthermore, it is assumed that the fine-scale HMM explaining $(X^*_{t,t^*})_{t^*}$ only depends on $S_t$. 

## References