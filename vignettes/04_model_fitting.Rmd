---
title: "Model fitting"
author: "Lennart OelschlÃ¤ger, Timo Adam and Rouven Michels"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model fitting}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Likelihood evaluation and numerical maximization

Timo

Conceptually, an HHMM can be treated as an HMM with two conditionally independent observations, the coarse-scale observation on the one hand and the corresponding chunk of fine-scale observations connected to a fine-scale HMM on the other hand. To derive the likelihood of an HHMM, we start by stating the likelihood formula for the fine-scale HMMs. 

Assume that we want to fit the $i$-th fine-scale HMM, with model parameters $\theta^{*(i)}=(\delta^{*(i)},\Gamma^{*(i)},(f^{*(i,k)})_k)$, to the $t$-th chunk of fine-scale observations, $(X_{t,t^*})_{t^*}$. Consider the so-called fine-scale forward probabilities 
$\alpha^{*(i)}_{k,t^*}=f^{*(i)}(X^*_{t,1},\dots,X^*_{t,t^*}, S^*_{t,t^*}=k)$,
where $t^*=1,\dots,T^*$ and $k=1,\dots,N^*$. Obviously,
\begin{align*}
\mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})=\sum_{k=1}^{N^*}\alpha^{*(i)}_{k,T^*}.
\end{align*}
The forward probabilities can be calculated in a recursive way of linear complexity:
\begin{align*}
\alpha^{*(i)}_{k,1} = \delta^{*(i)}_k f^{*(i,k)}(X^*_{t,1}) \quad \text{and} \quad
\alpha^{*(i)}_{k,t^*} = f^{*(i,k)}(X^*_{t,t^*})\sum_{j=1}^{N^*}\gamma^{*(i)}_{jk}\alpha^{*(i)}_{j,t^*-1}, \quad t^*=2,\dots,T^*.
\end{align*}

The transition from the likelihood function of an HMM to the likelihood function of an HHMM is straightforward: Consider the so-called coarse-scale forward probabilities
$\alpha_{i,t}=f(X_1,\dots,X_t,(X^*_{1,t^*})_{t^*},\dots,(X^*_{t,t^*})_{t^*}, S_t=i)$,
where $t=1,\dots,T$ and $i=1,\dots,N$. The likelihood function of the HHMM results as
\begin{align*}
\mathcal{L}^\text{HHMM}(\theta,(\theta^{*(i)})_i\mid (X_t)_t,((X^*_{t,t^*})_{t^*})_t)=\sum_{i=1}^{N}\alpha_{i,T}.
\end{align*}
The coarse-scale forward probabilities can be calculated similarly by applying the recursive scheme
\begin{align*}
\alpha_{i,1} &= \delta_i \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{1,t^*})_{t^*})f^{(i)}(X_1), \\
\alpha_{i,t} &= \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*}) f^{(i)}(X_t)\sum_{j=1}^{N}\gamma_{ji}\alpha_{j,t-1}, \quad t=2,\dots,T.
\end{align*}

## Challenges in the maximization of the likelihood
Maximization of the likelihood function is numerically feasible using the Newton-Raphson method. In practice, we often face the technical issues such as numerical under- or overflow, which can be addressed by maximizing the logarithm of the likelihood and incorporating constants in a conducive way. Instead of computing the forward probabilities directly, we consider the logarithmic transformation 
$\phi^{*(i)}_{k,t^*}=\log[\alpha^{*(i)}_{k,t^*}]$ and $\phi_{i,t}=\log[\alpha_{i,t}]$
thereof (log-forward probabilities). The recursive form described above remains: The fine-scale log-forward probabilities satisfy
\begin{align*}
\phi^{*(i)}_{k,1}&=\log[\delta^{*(i)}_k]+\log[f^{*(i,k)}(X^*_{t,1})], \\
\phi^{*(i)}_{k,t^*}&=\log[f^{*(i,k)}(X^*_{t,t^*})]+\log\left[\sum_{j=1}^{N^*} \gamma^{*(i)}_{jk} \exp[ \phi^{*(i)}_{j,t^*-1} -c_{t^*-1}]\right]+c_{t^*-1},
\end{align*}
where $c_{t^*-1}=\max \{ \phi^{*(i)}_{1,t^*-1},\dots,\phi^{*(i)}_{N^*,t^*-1} \}$ and $t^*=2,\dots,T^*$. The log-likelihood of a fine-scale HMM results from these variables as 
\begin{align*}
\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})=\log \left[ \sum_{k=1}^{N^*}\exp[\phi^{*(i)}_{k,T^*}-c_{T^*}]\right]+c_{T^*},
\end{align*}
where $c_{T^*} = \max\{ \phi^{*(i)}_{1,T^*},\dots,\phi^{*(i)}_{N^*,T^*} \}$. See Algorithm \ref{alg:llhmm} in the appendix for pseudo-code of the computation. The coarse-scale log-forward probabilities satisfy
\begin{align*}
\phi_{i,1}&=\log[\delta_i]+\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{1,t^*})_{t^*})+\log[f^{(i)}(X_{1})], \\
\phi_{i,t}&=\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})+\log[f^{(i)}(X_t)]+\log\left[\sum_{j=1}^{N}\gamma_{ji}\exp[\phi_{j,t-1}-c_{t-1}]\right]+c_{t-1},
\end{align*}
where $c_{t-1} = \max\{ \phi_{1,t-1},\dots,\phi_{N,t-1} \}$ and $t=2,\dots,T$. The log-likelihood of the HHMM results from these variables as 
\begin{align*}
\log \mathcal{L}^\text{HHMM}(\theta,(\theta^{*(i)})_i\mid (X_t)_t,((X^*_{t,t^*})_{t^*})_t)=\log\left[\sum_{i=1}^{N}\exp[\phi_{i,T}-c_{T}]\right]+c_{T},
\end{align*}
where $c_{T} = \max\{ \phi_{1,T},\dots,\phi_{N,T} \}$. See Algorithm \ref{alg:llhhmm} in the appendix for a pseudo-code. 

Additionally, we have to consider that certain model parameters must satisfy constraints, namely the transition probabilities and potentially parameters of the state-dependent distributions. Using parameter transformations serves the purpose. To ensure that the entries of the t.p.m.s fulfill non-negativity and the unity condition, we use a bijective transformation from the real numbers to the unity interval. Rather than estimating the probabilities $(\gamma_{ij})_{i,j}$ directly, we estimate unconstrained values $(\eta_{ij})_{i\neq j}$ for the non-diagonal entries of $\Gamma$ and derive the probabilities using the multinomial logit link
\begin{align*}
\gamma_{ij}=\frac{\exp[\eta_{ij}]}{1+\sum_{k\neq i}\exp[\eta_{ik}]},~i\neq j.
\end{align*}
The diagonal entries result from the unity condition $\gamma_{ii}=1-\sum_{j\neq i}\gamma_{ij}$. Noteworthy, not $N^2$ but $N(N-1)$ parameters have to be estimated for an $N\times N$-t.p.m. Furthermore, variances are strictly positive, which can be achieved by applying an exponential transformation to the unconstrained estimator. For basic HMMs, \cite{ale16} show that identifiability holds when the state-dependent distributions are distinct and the t.p.m.\ is ergodic and has full rank, conditions that are usually fulfilled in practice. Given that the fine-scale HMMs are distinct, this also holds for HHMMs (for a discussion of identifiability in HHMMs, see \citealp{leo19}).

A third source of conflicts arises from the fact that the likelihood is maximized with respect to a relatively large number of parameters, which can lead to local maxima apart from the global maximum. Common Newton-Raphson-type optimization routines are unable to distinguish local maxima from the global one. To avoid the trap of ending up at a local maximum, we recommended to run the maximization routine multiple times from different, possibly randomly selected starting points, and to choose the model that corresponds to the highest likelihood. A reasonable set of starting points can be chosen based on the observed data, e.g.\ using the method of moments estimator. Due to the increasingly complex likelihood surface, the number of optimization runs should increase with the number of parameters.


## Application

Lennart