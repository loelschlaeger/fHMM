---
title: "Model estimation"
author: "Lennart OelschlÃ¤ger, Timo Adam, and Rouven Michels"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
---

<!-- 
Assignee: Timo (theory part), Lennart (application part)
Purpose: definition of likelihood, maximization of likelihood, challenges in the maximization, application in fHMM
--> 

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Likelihood evaluation using the forward algorithm

An HHMM can be treated as an HMM with two conditionally independent data streams; the coarse-scale observations on the one hand and the corresponding chunk of fine-scale observations connected to a fine-scale HMM on the other hand. To derive the likelihood of an HHMM, we start by computing the likelihood of each chunk of fine-scale observations being generated by each fine-scale HMM. 

To fit the $i$-th fine-scale HMM, with model parameters $\theta^{*(i)}=(\delta^{*(i)}, \Gamma^{*(i)},(f^{*(i,k)})_k)$ to the $t$-th chunk of fine-scale observations, which is denoted by $(X_{t,t^*})_{t^*}$, we consider the fine-scale forward probabilities 
\begin{align*}
\alpha^{*(i)}_{k,t^*}=f^{*(i)}(X^*_{t,1},\dots,X^*_{t,t^*}, S^*_{t,t^*}=k),
\end{align*}
where $t^*=1,\dots,T^*$ and $k=1,\dots,N^*$. Using the fine-scale forward probabilities, the fine-scale likelihoods can be obtained from the law of total probability as
\begin{align*}
\mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})=\sum_{k=1}^{N^*}\alpha^{*(i)}_{k,T^*}.
\end{align*}
The forward probabilities can be calculated in a recursively as
\begin{align*}
\alpha^{*(i)}_{k,1} &= \delta^{*(i)}_k f^{*(i,k)}(X^*_{t,1}), \\
\alpha^{*(i)}_{k,t^*} &= f^{*(i,k)}(X^*_{t,t^*})\sum_{j=1}^{N^*}\gamma^{*(i)}_{jk}\alpha^{*(i)}_{j,t^*-1}, \quad t^*=2,\dots,T^*.
\end{align*}

The transition from the likelihood function of an HMM to the likelihood function of an HHMM is straightforward: Consider the coarse-scale forward probabilities
$\alpha_{i,t}=f(X_1,\dots,X_t,(X^*_{1,t^*})_{t^*},\dots,(X^*_{t,t^*})_{t^*}, S_t=i)$,
where $t=1,\dots,T$ and $i=1,\dots,N$. The likelihood function of the HHMM results as
\begin{align*}
\mathcal{L}^\text{HHMM}(\theta,(\theta^{*(i)})_i\mid (X_t)_t,((X^*_{t,t^*})_{t^*})_t)=\sum_{i=1}^{N}\alpha_{i,T}.
\end{align*}
The coarse-scale forward probabilities can be calculated similarly by applying the recursive scheme
\begin{align*}
\alpha_{i,1} &= \delta_i \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{1,t^*})_{t^*})f^{(i)}(X_1), \\
\alpha_{i,t} &= \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*}) f^{(i)}(X_t)\sum_{j=1}^{N}\gamma_{ji}\alpha_{j,t-1}, \quad t=2,\dots,T.
\end{align*}

## Challenges associated with the likelihood maximization

Maximization of the likelihood function is numerically feasible using the Newton-Raphson method. In practice, we often face technical issues such as numerical under- or overflow, which can be addressed by maximizing the logarithm of the likelihood and incorporating constants in a conducive way. Instead of computing the forward probabilities directly, we consider a logarithmic transformation.

Additionally, we have to consider that certain model parameters must satisfy constraints, namely the transition probabilities and potentially parameters of the state-dependent distributions. Using parameter transformations serves the purpose. To ensure that the entries of the t.p.m.s fulfill non-negativity and the unity condition, we use a bijective transformation from the real numbers to the unity interval. Rather than estimating the probabilities $(\gamma_{ij})_{i,j}$ directly, we estimate unconstrained values $(\eta_{ij})_{i\neq j}$ for the non-diagonal entries of $\Gamma$ and derive the probabilities using the multinomial logit link
\begin{align*}
\gamma_{ij}=\frac{\exp[\eta_{ij}]}{1+\sum_{k\neq i}\exp[\eta_{ik}]},~i\neq j.
\end{align*}
The diagonal entries result from the unity condition $\gamma_{ii}=1-\sum_{j\neq i}\gamma_{ij}$. Noteworthy, not $N^2$ but $N(N-1)$ parameters have to be estimated for an $N\times N$-t.p.m. Furthermore, variances are strictly positive, which can be achieved by applying an exponential transformation to the unconstrained estimator.

A third source of conflicts arises from the fact that the likelihood is maximized with respect to a relatively large number of parameters, which can lead to local maxima apart from the global maximum. Common Newton-Raphson-type optimization routines are unable to distinguish local maxima from the global one. To avoid the trap of ending up at a local maximum, we recommended to run the maximization routine multiple times from different, possibly randomly selected starting points, and to choose the model that corresponds to the highest likelihood. A reasonable set of starting points can be chosen based on the observed data, e.g.\ using the method of moments estimator. Due to the increasingly complex likelihood surface, the number of optimization runs should increase with the number of parameters.

## Application

## References
