---
title: "Model definition"
author: "Lennart OelschlÃ¤ger, Timo Adam and Rouven Michels"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model definition}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

<!-- 
Assignee: Rouven
Purpose: definition of HMMs, definition of hierarchical structure
--> 
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## The Hidden Markov Model
Hidden Markov models (HMMs) are a modeling framework for time series where a sequence of observation is assumed to depend on a latent state process. The peculiarity is that, instead of the observation process, this state process cannot be directly observed. However, the latent states comprise information about the environment the model is applied on. The connection between hidden state process and observed state-dependent process arises by the the following:

Let $N$ be the number of states, we assume that for each point in time $t = 1, \ldots, T$, an underlying process $(S_t)_t$ selects one of those $N$ states. Then, depending on the active state $S_t = i$, $i = 1, \ldots, N$, the observation $X_t$ from the state-dependent process $(X_t)_t$ is generated by one of the $N$ distributions $f^{(1)},\dots,f^{(N)}.$

Furthermore, we assume $(S_t)_t$ to be Markovian, i.e. the actual state only depends on the previous state. Henceforth, we can identify the process by its initial distribution $\delta$ and its transition probability matrix (t.p.m.) $\Gamma$. Moreover, by construction, we force the process $(X_t)_t$ to satisfy the conditional independence assumption, i.e.\ the actual observation depends on the current state $S_t$, but does not depend on previous observations or states at all.

Referring to financial data, the different states can serve as proxys for the actual market situation, e.g. calm or nervous. As such a mood can not directly be observed, we make use of observable data, e.g. price changes of stocks. We assume that these price changes depend on the current mood. In particular, we assume price changes to be generated by different normal distribution whose mean (and in special cases its volatility) depend on the actual state $S_t = i$.

Following @zuc16 we also assume that the initial distribution $\delta$ equals the (assumed existing) stationary distribution $\pi$ where $\pi = \pi \Gamma$, i.e. the stationary and henceforth the initial distribution is determined by the transition probability matrix $\Gamma$. Doing so we reduce the number of parameters to be estimated.

## Adding a Hierarchical Structure

The structure of HMMs can easily be flexibilized. For instance, an hierarchical structure (HMMM) which consists of two processes observed on two different time scales can be implemented. The two processes, one on a coarser and one on a finer scale differ in the number of observations, e.g. monthly observations on the coarser scale and daily or weekly observations on the finer scale. We model both time series in an hierarchical manner. In particular, we assume that the time series on the coarser scale is generated by an ordinary HMM, i.e. it adopts all the properties of the HMM displayed above. Furthermore, we collect the observations fo the fine-scale time series into $T$ areas such that every fine-scale observation corresponds to the according coarse-scale time point. 

$\textbf{noch alt}$
Subsequently, we segment the observations of the fine-scale time series into $T$ distinct chunks, each of which contains all data points that correspond to the $t$-th coarse-scale time point. Assuming that we have $T^*$ fine-scale observations on every coarse-scale time point, we face $T$ chunks comprising of $T^*$ fine-scale observations each. The hierarchical structure now evinces itself as we model each of the chunks by one of $N$ possible fine-scale HMMs. Each of the fine-scale HMMs has its own t.p.m.\ $\Gamma^{*(i)}$, initial distribution $\delta^{*(i)}$, stationary distribution $\pi^{*(i)}$, and state-dependent distributions $f^{*(i,1)},\dots,f^{*(i,N^*)}$. Which fine-scale HMM is selected to explain the $t$-th chunk of fine-scale observations depends on the hidden coarse-scale state $S_t$. The $i$-th fine-scale HMM explaining the $t$-th chunk of fine-scale observations consists of the following two stochastic processes:

1. At each time point $t^*$ of the fine-scale time space $\{1,\dots,T^*\}$, the process $(S^*_{t,t^*})_{t^*}$ selects one state from the fine-scale state space $\{1,\dots,N^*\}$. We call $(S^*_{t,t^*})_{t^*}$ the hidden fine-scale state process.

2. Depending on which state is active at $t^*$, one of $N^*$ distributions $f^{*(i,1)},\dots,f^{*(i,N^*)}$ realizes the observation $X^*_{t,t^*}$. The process $(X^*_{t,t^*})_{t^*}$ is called the observed fine-scale state-dependent process. 

The fine-scale processes $(S^*_{1,t^*})_{t^*},\dots,(S^*_{T,t^*})_{t^*}$ and $(X^*_{1,t^*})_{t^*},\dots,(X^*_{T,t^*})_{t^*}$ satisfy the Markov property and the conditional independence assumption, respectively, as well. Furthermore, it is assumed that the fine-scale HMM explaining $(X^*_{t,t^*})_{t^*}$ only depends on $S_t$. 

## References