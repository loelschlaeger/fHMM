---
title: "Model definition"
author: "Lennart Oelschläger, Timo Adam and Rouven Michels"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model definition}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
link-citations: true
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

<!-- 
Assignee: Rouven
Purpose: definition of HMMs, definition of hierarchical structure
--> 
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## The Hidden Markov Model
Hidden Markov models (HMMs) are a modelling framework for time series where a sequence of observation is assumed to depend on a latent state process. The peculiarity is that, instead of the observation process, the state process cannot be directly observed. However, the latent states comprise information about the environment the model is applied on. The connection between hidden state process and observed state-dependent process arises by the following:

Let $N$ be the number of possible states. We assume that for each point in time $t = 1, \ldots, T$, an underlying process $(S_t)_{t = 1, \ldots, T}$ selects one of those $N$ states. Then, depending on the active state $S_t = i$, $i = 1, \ldots, N$, the observation $X_t$ from the state-dependent process $(X_t)_{t = 1, \ldots, T}$ is generated by one of the $N$ distributions $f^{(1)},\dots,f^{(N)}.$

Furthermore, we assume $(S_t)_t$ to be Markovian, i.e. we assume that the actual state only depends on the previous state. Henceforth, we can identify the process by its initial distribution $\delta$ and its transition probability matrix (t.p.m.) $\Gamma$. Moreover, by construction, we force the process $(X_t)_{t = 1, \ldots, T}$ to satisfy the conditional independence assumption, i.e.\ the actual observation $X_t$ depends on the current state $S_t$, but does not depend on previous observations or states at all.

Referring to financial data, the different states can serve as proxys for the actual market situation, e.g. calm or nervous. As such a mood can not directly be observed, we make use of observable data, e.g. price changes of stocks and assume that these price changes depend on the current mood. In particular, we assume price changes to be generated by different normal distribution whose mean (and in special cases its volatility) depend on the actual state $S_t = i$.

Following @zuc16 we also assume that the initial distribution $\delta$ equals the (assumed existing) stationary distribution $\pi$ where $\pi = \pi \Gamma$, i.e. the stationary and henceforth the initial distribution is determined by the transition probability matrix $\Gamma$. Doing so we reduce the number of parameters to be estimated.

## Adding a Hierarchical Structure

The structure of HMMs can easily be flexibilized. For instance, an hierarchical structure which consists of two processes observed on two different time scales can be implemented, named hierarchical hidden Markov model (HMMM). The two processes, one on a coarser and one on a finer scale differ in the number of observations, e.g. monthly observations on the coarser scale and daily or weekly observations on the finer scale. We model both time series in an hierarchical manner. In particular, we assume that the time series on the coarser scale is generated by an ordinary HMM, i.e. it adopts all the properties of the HMM displayed above. 

Furthermore, we collect the observations of the fine-scale time series into $T$ areas such that every fine-scale observation corresponds to one according coarse-scale time point $t = 1, \ldots, T$. Then, every chunk referring to one coarse-scale time point contains $T^*$ fine-scale observations each. This correspondence is the basis for modelling the fine-scale $N$-state HMM with the state-dependent distributions $f^{*(i,1)},\dots,f^{*(i,N^*)}$, the transition probability matrix $\Gamma^{*(i)}$ and the stationary distribution $\pi^{*(i)}$ (i.e., see above, equal to the initial distribution $\delta^{*(i)}$). The hidden coarse-scale state $S_t$ determines the selection of the $i$-th fine-scale HMM in order to explain the fine-scale observations in time chunk $t$. In particular, for every $t^* = 1, \ldots, T^*$ the hidden fine-scale process $(S^*_{t,t^*})_{t^* = 1, \ldots, T^*}$ is selecting of $N^*$ states. Then, the active state at time $t^*$ determines the selection of one distribution of the $N^*$ possible distributions $\{f^{*(i,1)},\dots,f^{*(i,N^*)}\}$. The $t^*$-th realization $X^*_{t,t^*}$ of the observed fine-scale state-dependent process $(X^*_{t,t^*})_{t^* = 1, \ldots, T^*}$ stems from the choice of this distribution.
Due to its analogous construction to regular hidden Markov frameworks, the hidden fine-scale process is Markovian and the state-dependent process fulfils the conditional independence assumption.
To give the reader a better overview of the hierarchical hidden Markov model (HMM), Figure $\color{red}{hier\ noch\ Figure\ einfügen\ falls\ erwünscht}$ displays the relationship between the coarse and fine scale processes.  

## References

