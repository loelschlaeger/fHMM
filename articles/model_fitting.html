<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model fitting • fHMM</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/simplex/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Model fitting">
<meta property="og:description" content="fHMM">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">fHMM</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.3.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">
    <span class="fa fa-clipboard"></span>
     
    Reference
  </a>
</li>
<li>
  <a href="../news/index.html">
    <span class="fa fa-wrench"></span>
     
    Changelog
  </a>
</li>
<li>
  <a href="https://github.com/loelschlaeger/fHMM" class="external-link">
    <span class="fa fa-github fa-lg"></span>
     
    GitHub Repository
  </a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/loelschlaeger/fHMM/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="model_fitting_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Model fitting</h1>
                        <h4 data-toc-skip class="author">Lennart Oelschläger, Timo Adam and Rouven Michels</h4>
            
            <h4 data-toc-skip class="date">2021-12-07</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/loelschlaeger/fHMM/blob/HEAD/vignettes/model_fitting.Rmd" class="external-link"><code>vignettes/model_fitting.Rmd</code></a></small>
      <div class="hidden name"><code>model_fitting.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="likelihood-evaluation-and-numerical-maximization">Likelihood evaluation and numerical maximization<a class="anchor" aria-label="anchor" href="#likelihood-evaluation-and-numerical-maximization"></a>
</h2>
<p>Conceptually, an HHMM can be treated as an HMM with two conditionally independent observations, the coarse-scale observation on the one hand and the corresponding chunk of fine-scale observations connected to a fine-scale HMM on the other hand. To derive the likelihood of an HHMM, we start by stating the likelihood formula for the fine-scale HMMs.</p>
<p>Assume that we want to fit the <span class="math inline">\(i\)</span>-th fine-scale HMM, with model parameters <span class="math inline">\(\theta^{*(i)}=(\delta^{*(i)},\Gamma^{*(i)},(f^{*(i,k)})_k)\)</span>, to the <span class="math inline">\(t\)</span>-th chunk of fine-scale observations, <span class="math inline">\((X_{t,t^*})_{t^*}\)</span>. Consider the so-called fine-scale forward probabilities <span class="math inline">\(\alpha^{*(i)}_{k,t^*}=f^{*(i)}(X^*_{t,1},\dots,X^*_{t,t^*}, S^*_{t,t^*}=k)\)</span>, where <span class="math inline">\(t^*=1,\dots,T^*\)</span> and <span class="math inline">\(k=1,\dots,N^*\)</span>. Obviously, <span class="math display">\[\begin{align*}
\mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})=\sum_{k=1}^{N^*}\alpha^{*(i)}_{k,T^*}.
\end{align*}\]</span> The forward probabilities can be calculated in a recursive way of linear complexity: <span class="math display">\[\begin{align*}
\alpha^{*(i)}_{k,1} = \delta^{*(i)}_k f^{*(i,k)}(X^*_{t,1}) \quad \text{and} \quad
\alpha^{*(i)}_{k,t^*} = f^{*(i,k)}(X^*_{t,t^*})\sum_{j=1}^{N^*}\gamma^{*(i)}_{jk}\alpha^{*(i)}_{j,t^*-1}, \quad t^*=2,\dots,T^*.
\end{align*}\]</span></p>
<p>The transition from the likelihood function of an HMM to the likelihood function of an HHMM is straightforward: Consider the so-called coarse-scale forward probabilities <span class="math inline">\(\alpha_{i,t}=f(X_1,\dots,X_t,(X^*_{1,t^*})_{t^*},\dots,(X^*_{t,t^*})_{t^*}, S_t=i)\)</span>, where <span class="math inline">\(t=1,\dots,T\)</span> and <span class="math inline">\(i=1,\dots,N\)</span>. The likelihood function of the HHMM results as <span class="math display">\[\begin{align*}
\mathcal{L}^\text{HHMM}(\theta,(\theta^{*(i)})_i\mid (X_t)_t,((X^*_{t,t^*})_{t^*})_t)=\sum_{i=1}^{N}\alpha_{i,T}.
\end{align*}\]</span> The coarse-scale forward probabilities can be calculated similarly by applying the recursive scheme <span class="math display">\[\begin{align*}
\alpha_{i,1} &amp;= \delta_i \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{1,t^*})_{t^*})f^{(i)}(X_1), \\
\alpha_{i,t} &amp;= \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*}) f^{(i)}(X_t)\sum_{j=1}^{N}\gamma_{ji}\alpha_{j,t-1}, \quad t=2,\dots,T.
\end{align*}\]</span></p>
</div>
<div class="section level2">
<h2 id="challenges-in-the-maximization-of-the-likelihood">Challenges in the maximization of the likelihood<a class="anchor" aria-label="anchor" href="#challenges-in-the-maximization-of-the-likelihood"></a>
</h2>
<p>Maximization of the likelihood function is numerically feasible using the Newton-Raphson method. In practice, we often face the technical issues such as numerical under- or overflow, which can be addressed by maximizing the logarithm of the likelihood and incorporating constants in a conducive way. Instead of computing the forward probabilities directly, we consider the logarithmic transformation <span class="math inline">\(\phi^{*(i)}_{k,t^*}=\log[\alpha^{*(i)}_{k,t^*}]\)</span> and <span class="math inline">\(\phi_{i,t}=\log[\alpha_{i,t}]\)</span> thereof (log-forward probabilities). The recursive form described above remains: The fine-scale log-forward probabilities satisfy <span class="math display">\[\begin{align*}
\phi^{*(i)}_{k,1}&amp;=\log[\delta^{*(i)}_k]+\log[f^{*(i,k)}(X^*_{t,1})], \\
\phi^{*(i)}_{k,t^*}&amp;=\log[f^{*(i,k)}(X^*_{t,t^*})]+\log\left[\sum_{j=1}^{N^*} \gamma^{*(i)}_{jk} \exp[ \phi^{*(i)}_{j,t^*-1} -c_{t^*-1}]\right]+c_{t^*-1},
\end{align*}\]</span> where <span class="math inline">\(c_{t^*-1}=\max \{ \phi^{*(i)}_{1,t^*-1},\dots,\phi^{*(i)}_{N^*,t^*-1} \}\)</span> and <span class="math inline">\(t^*=2,\dots,T^*\)</span>. The log-likelihood of a fine-scale HMM results from these variables as <span class="math display">\[\begin{align*}
\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})=\log \left[ \sum_{k=1}^{N^*}\exp[\phi^{*(i)}_{k,T^*}-c_{T^*}]\right]+c_{T^*},
\end{align*}\]</span> where <span class="math inline">\(c_{T^*} = \max\{ \phi^{*(i)}_{1,T^*},\dots,\phi^{*(i)}_{N^*,T^*} \}\)</span>. See Algorithm  in the appendix for pseudo-code of the computation. The coarse-scale log-forward probabilities satisfy <span class="math display">\[\begin{align*}
\phi_{i,1}&amp;=\log[\delta_i]+\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{1,t^*})_{t^*})+\log[f^{(i)}(X_{1})], \\
\phi_{i,t}&amp;=\log \mathcal{L}^\text{HMM}(\theta^{*(i)}\mid (X^*_{t,t^*})_{t^*})+\log[f^{(i)}(X_t)]+\log\left[\sum_{j=1}^{N}\gamma_{ji}\exp[\phi_{j,t-1}-c_{t-1}]\right]+c_{t-1},
\end{align*}\]</span> where <span class="math inline">\(c_{t-1} = \max\{ \phi_{1,t-1},\dots,\phi_{N,t-1} \}\)</span> and <span class="math inline">\(t=2,\dots,T\)</span>. The log-likelihood of the HHMM results from these variables as <span class="math display">\[\begin{align*}
\log \mathcal{L}^\text{HHMM}(\theta,(\theta^{*(i)})_i\mid (X_t)_t,((X^*_{t,t^*})_{t^*})_t)=\log\left[\sum_{i=1}^{N}\exp[\phi_{i,T}-c_{T}]\right]+c_{T},
\end{align*}\]</span> where <span class="math inline">\(c_{T} = \max\{ \phi_{1,T},\dots,\phi_{N,T} \}\)</span>. See Algorithm  in the appendix for a pseudo-code.</p>
<p>Additionally, we have to consider that certain model parameters must satisfy constraints, namely the transition probabilities and potentially parameters of the state-dependent distributions. Using parameter transformations serves the purpose. To ensure that the entries of the t.p.m.s fulfill non-negativity and the unity condition, we use a bijective transformation from the real numbers to the unity interval. Rather than estimating the probabilities <span class="math inline">\((\gamma_{ij})_{i,j}\)</span> directly, we estimate unconstrained values <span class="math inline">\((\eta_{ij})_{i\neq j}\)</span> for the non-diagonal entries of <span class="math inline">\(\Gamma\)</span> and derive the probabilities using the multinomial logit link <span class="math display">\[\begin{align*}
\gamma_{ij}=\frac{\exp[\eta_{ij}]}{1+\sum_{k\neq i}\exp[\eta_{ik}]},~i\neq j.
\end{align*}\]</span> The diagonal entries result from the unity condition <span class="math inline">\(\gamma_{ii}=1-\sum_{j\neq i}\gamma_{ij}\)</span>. Noteworthy, not <span class="math inline">\(N^2\)</span> but <span class="math inline">\(N(N-1)\)</span> parameters have to be estimated for an <span class="math inline">\(N\times N\)</span>-t.p.m. Furthermore, variances are strictly positive, which can be achieved by applying an exponential transformation to the unconstrained estimator. For basic HMMs,  show that identifiability holds when the state-dependent distributions are distinct and the t.p.m. is ergodic and has full rank, conditions that are usually fulfilled in practice. Given that the fine-scale HMMs are distinct, this also holds for HHMMs (for a discussion of identifiability in HHMMs, see ).</p>
<p>A third source of conflicts arises from the fact that the likelihood is maximized with respect to a relatively large number of parameters, which can lead to local maxima apart from the global maximum. Common Newton-Raphson-type optimization routines are unable to distinguish local maxima from the global one. To avoid the trap of ending up at a local maximum, we recommended to run the maximization routine multiple times from different, possibly randomly selected starting points, and to choose the model that corresponds to the highest likelihood. A reasonable set of starting points can be chosen based on the observed data, e.g. using the method of moments estimator. Due to the increasingly complex likelihood surface, the number of optimization runs should increase with the number of parameters.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Lennart Oelschläger, Timo Adam, Michels Rouven.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.1.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
